# Distillation of reasoning into small language models

Мы с командой делали проект в рамках студкемпа по обработке естественного языка, проводимого Яндексом и НГУ. Нашей задачей было научить малые языковые модели размышлять. Здесь предоставлены результаты моей работы над подходом Socratic Chain-of-Thoughts, который я взяла из [статьи](https://aclanthology.org/2023.findings-acl.441.pdf). Идея в том, чтобы дообучить две маленькие модели. Первая должна декомпозировать поступающий вопрос на подзадачи, а вторая должна научиться решать подзадачи.

Маленькие модели: Qwen3-1.7B
Модель-учитель: Qwen3-235B-A22B
Датасет: взяли вопросы из GSM8K, с помощью Qwen3-235B-A22B перевели их на русский язык, разбили на подзадачи и решения подзадач.

Я занималась сбором данных, обучением модели, решающей подзадачи, сведением двух моделей в общий пайплайн и оценкой этого подхода на датасетах MERA. Код всего этого доступен в репозитории
