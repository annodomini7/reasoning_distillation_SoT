{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Data aggregation","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade cerebras_cloud_sdk -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom tqdm.notebook import tqdm, trange\nimport json\nimport re\nfrom cerebras.cloud.sdk import Cerebras\n\ndataset = load_dataset(\"openai/gsm8k\", 'socratic', split=\"train\")\nquestions = [dataset[i]['question'] for i in trange(len(dataset))]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nn = len(questions)\noutput_path = \"dataset_ru1_updated.json\"\n\nclient = Cerebras(\n    api_key=user_secrets.get_secret(\"data_apis\"),\n)\n\nfor i in trange(n):\n    response = client.chat.completions.create(\n                messages=[\n                    {\n                        \"role\": \"user\",\n                        \"content\": \"\"\"Вы — опытный переводчик и решатель задач.\nПереведите текст на русский, сохранив математические формулы и структуру без изменений.\nПосле этого выведи последовательные вопросы и ответы на них, позволяющие решить задачу.\nФормат вывода: 'Q: Условие задачи на русском языке \\n SQ1: первый подвопрос \\n A1: ответ на первый подвопрос\\n ... A: ответ на задачу'.\nНапример, 'Q:У Кати было 3 кружки, у Кости 5 кружек. Сколько суммарно у них было кружек? \\n SQ1: Сколько будет 3 плюс 5? \\n A1: 3+5=8, значит будет 8 \\n A: Ответ: будет 8 кружек. \\n '.\\n\"\"\" + questions[i],\n                    }\n            ],\n                model=\"qwen-3-235b-a22b\",\n            )\n    answer = response.choices[0].message.content.strip()\n    # answer = answer.split(\"</think>\")[1].lsplit().split()\n    with open(output_path, \"a\", encoding=\"utf-8\") as f:\n        f.write(json.dumps({\"answer\": answer.split(\"</think>\")[1]}, ensure_ascii=False) + \"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pattern = r'(SQ\\d+:\\s.*?)(?=\\n(?:SQ\\d+|$))'\n\ndef parse_qa(text):\n    text = text.strip()\n    # print(text)\n    if \"Q:\" in text:\n        _, rest = text.split(\"Q:\", 1)\n    else:\n        return None, [], \"\"\n        \n    if \"A:\" in rest:\n        question_part, final_answer = rest.split(\"A:\", 1)\n    else:\n        return None, [], \"\"\n        \n    if \"SQ\" in question_part:\n        question_part, answer_part = question_part.split(\"SQ\", 1)\n        answer_part = \"SQ\" + answer_part\n    else:\n        return None, [], \"\"\n    steps = re.split(r'(?:SQ\\d+:\\s|A:\\s)', answer_part.strip())\n    keys = re.findall(r'(SQ\\d+:\\s.*?)(?=(?:SQ|$))', answer_part, re.DOTALL)\n    \n    reasoning_steps = []\n    for key, step in zip(keys, steps[1:]):\n        match = re.search(r'SQ\\d+:\\s*(.*?)\\s*\\n\\s*A\\d+:\\s*(.*)', key, re.DOTALL)\n\n        if match:\n            question = match.group(1).strip()\n            answer = match.group(2).strip()\n            reasoning_steps.append({\n                \"step\": question,\n                \"answer\": answer\n            })\n    # final_answer = steps[-1].strip()\n\n    return question_part.strip(), reasoning_steps, final_answer.strip()\n\ntext = \"\\n\\nQ: Наталья продала 48 заколок своим подругам в апреле, а в мае она продала в два раза меньше. Сколько заколок Наталья продала всего в апреле и мае?  \\n SQ1: Как вычислить количество заколок, проданных в мае?  \\n A1: Количество проданных в мае заколок равно 48 ÷ 2 = 24.  \\n SQ2: Как найти общее количество заколок, проданных в апреле и мае?  \\n A2: Нужно сложить количество за апрель и май: 48 + 24 = 72.  \\n A: Ответ: Наталья продала всего 72 заколки.\"\nparse_qa(text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_files = [\"/kaggle/input/ru-gsm8k/dataset_ru1_updated(5).json\",\n               \"/kaggle/input/ru-gsm8k/dataset_ru1_updated(7).json\",\n               \"/kaggle/input/ru-gsm8k/dataset_ru1_updated(8).json\"]\noutput_file = \"structured_cot_dataset.jsonl\"\n\nfor file in input_files:\n    with open(file, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            item = json.loads(line.strip())\n            full_text = item[\"answer\"]\n            question, steps, answer = parse_qa(full_text)\n            if question and answer:\n                entry = {\n                    \"prompt\": question,\n                    \"reasoning_steps\": steps,\n                    \"final_answer\": answer\n                }\n                with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n                    out_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\nprint(f\"Структурированный датасет сохранён в {output_file}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### QA model education","metadata":{}},{"cell_type":"code","source":"!pip install \\\nflash-attn==2.7.4.post1 \\\nscipy==1.15.2 \\\ntorch==2.6.0 \\\ncffi==1.17.1 \\\ntransformers==4.53.2 \\\npeft==0.14.0 \\\naccelerate==1.5.1 \\\ntrl==0.19.1 \\\nbitsandbytes==0.45.3 \\\ndatasets==3.3.2 \\\nhuggingface-hub==0.33.4 \\\nsafetensors==0.5.3 \\\npandas==2.2.3 \\\nmatplotlib==3.10.1 \\\nnumpy==1.26.4 \\\nwandb\n-q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom trl import SFTConfig, SFTTrainer\nfrom tqdm.notebook import tqdm, trange\nimport wandb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.login(key=user_secrets.get_secret(\"wandb_api\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Device used: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    \"Qwen/Qwen3-1.7B\"\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen3-1.7B\",\n    attn_implementation=\"sdpa\",\n    torch_dtype=torch.bfloat16,\n    device_map=device\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"json\", data_files=\"/kaggle/input/ru-gsm8k-strustered/structured_dataset_train.jsonl\", split=\"train\")\n\ndef generate_progressive_examples(example):\n    prompt = example[\"prompt\"]\n    steps = example.get(\"reasoning_steps\", [])\n    final_answer = example.get(\"final_answer\", \"\")\n\n    examples = []\n    context = f\"{prompt}\\n\"\n\n    for i in range(len(steps)):\n        step = steps[i]\n        context += f\"{step['step']}\\n\"\n        \n        examples.append({\n            \"messages\": [{'content': 'Ты полезный ассистент. Если в запросе последнее предложение вопрос - надо ответить на него. Иначе надо дать итоговый ответ по задаче, основываясь на введенном тексте.','role': 'system'},\n                         {'content': context,'role': 'user'},\n                         {'content': f\"{step['answer']}\\n\",'role': 'assistant'}]\n        })\n        context += f\"{step['answer']}\\n\"\n    \n    if final_answer:\n        examples.append({\n            \"messages\": [{'content': 'Ты полезный ассистент. Если в запросе последнее предложение вопрос - надо ответить на него. Иначе надо дать итоговый ответ по задаче, основываясь на введенном тексте.','role': 'system'},\n                         {'content': context,'role': 'user'},\n                         {'content': f\"{final_answer}\\n\",'role': 'assistant'}]\n        })\n\n    return {\"progressive_examples\": examples}\n\ndataset_expanded = dataset.map(\n    generate_progressive_examples,\n    remove_columns=dataset.column_names,\n    batched=False\n)\n\nfrom itertools import chain\n\ndef flatten_dataset(batch):\n    return {\n        \"messages\": list(chain.from_iterable(batch[\"progressive_examples\"]))\n    }\n\ndataset_flat = dataset_expanded.map(\n    flatten_dataset,\n    batched=True,\n    remove_columns=[\"progressive_examples\"]\n)\ndf_train = dataset_flat.map(lambda x: x[\"messages\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_example(example):\n    messages = example[\"messages\"]\n\n    prompt_messages = messages[:-1]\n    assistant = messages[-1][\"content\"]\n\n    prompt = tokenizer.apply_chat_template(\n        prompt_messages,\n        add_generation_prompt=True,\n        tokenize=False\n    )\n\n    return {\"prompt\": prompt, \"completion\": assistant}\n\ndef tokenize_and_align_labels(example):\n    prompt = example[\"prompt\"]\n    completion = example[\"completion\"]\n\n    full_text = prompt + completion\n    inputs = tokenizer(full_text, truncation=True, max_length=1024)\n\n    prompt_len = len(tokenizer(prompt, add_special_tokens=False)[\"input_ids\"])\n    labels = [-100] * prompt_len + inputs[\"input_ids\"][prompt_len:]\n\n    inputs[\"labels\"] = labels\n    return inputs\n\ndf_train = df_train.map(convert_example, remove_columns=df_train.column_names)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### LoRA fine-tuning","metadata":{}},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\n\nconfig = LoraConfig(\n    # the rank of the adapter, the lower the fewer parameters you'll need to train\n    r=8,\n    lora_alpha=16, # multiplier, usually 2*r\n    bias=\"none\",\n    lora_dropout=0.05,\n    task_type=\"CAUSAL_LM\",\n    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n)\nmodel = get_peft_model(model, config)\nmodel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.print_trainable_parameters()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sft_config = SFTConfig(\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={'use_reentrant': False},\n    gradient_accumulation_steps=8,\n    per_device_train_batch_size=4,\n    # auto_find_batch_size=True,\n\n    max_seq_length=1024,\n    packing=False,\n    completion_only_loss=True,\n    # assistant_only_loss=True,\n\n    num_train_epochs=2,\n    learning_rate=3e-4,\n    optim='paged_adamw_8bit',\n\n    logging_steps=10,\n    logging_dir='./logs',\n    output_dir='./qwen_finetuned_lora',\n    report_to=[\"wandb\"],\n\n    save_strategy='epoch',\n    save_total_limit=2,\n    bf16=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shuffled_train = df_train.shuffle()\ndf_train_part = shuffled_train.select(range(20000))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_lora = SFTTrainer(\n    model=model,\n    processing_class=tokenizer,\n    args=sft_config,\n    train_dataset=df_train_part,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_lora.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/results.zip /kaggle/working/qwen_finetuned_lora","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Two models","metadata":{}},{"cell_type":"code","source":"class TwoStageModel:\n    def __init__(self, model_qg_path, model_qa_path):\n        self.model_qg = AutoModelForCausalLM.from_pretrained(\n            model_qg_path,\n            # attn_implementation=\"sdpa\",\n            torch_dtype=torch.bfloat16,\n            device_map=device\n        )\n        self.model_qa = AutoModelForCausalLM.from_pretrained(\n            model_qa_path,\n            # attn_implementation=\"sdpa\",\n            torch_dtype=torch.bfloat16,\n            device_map=device\n        )\n        \n        self.tokenizer_qg = AutoTokenizer.from_pretrained(model_qg_path)\n        self.tokenizer_qa = AutoTokenizer.from_pretrained(model_qa_path)\n        \n        self.model_qg.eval()\n        self.model_qa.eval()\n\n        self.QG_PROMPT = \"Ты полезный ассистент. Тебе вводят задачу и существующие этапы решения. Выведи следующий вопрос, помогающий решить задачу.\"\n        self.QA_PROMPT = \"Ты полезный ассистент. Если в запросе последнее предложение вопрос - надо ответить на него. Иначе надо дать итоговый ответ по задаче, основываясь на введенном тексте, в формате: 'Ответ: *твой ответ*'.\"\n    \n    def gen_prompt(self, tokenizer, sentence, system_prompt):\n        converted_sample = [\n            {'content': system_prompt, 'role': 'system'},\n            {'content': sentence, 'role': 'user'}\n        ]\n        prompt = tokenizer.apply_chat_template(\n            converted_sample, tokenize=False, add_generation_prompt=True\n        )\n        return prompt\n        \n    def generate(self, prompt, max_new_tokens=100, **kwargs):\n        for _ in range(10):\n            inputs1 = self.tokenizer_qg(self.gen_prompt(self.tokenizer_qg, prompt, self.QG_PROMPT),\n                                        return_tensors=\"pt\").to(self.model_qg.device)\n            \n            with torch.no_grad():\n                output1 = self.model_qg.generate(\n                    **inputs1,\n                    max_new_tokens=max_new_tokens,\n                    **kwargs\n                )\n            \n            intermediate_text = self.tokenizer_qg.decode(output1[0, inputs1['input_ids'].shape[1]:],\n                                                         skip_special_tokens=True)\n            intermediate_text = intermediate_text.replace(\"<think>\", \"\")\n            intermediate_text = intermediate_text.replace(\"</think>\", \"\")\n            \n            prompt += intermediate_text\n            inputs2 = self.tokenizer_qa(self.gen_prompt(self.tokenizer_qa, prompt, self.QA_PROMPT),\n                                        return_tensors=\"pt\").to(self.model_qa.device)\n            \n            with torch.no_grad():\n                output2 = self.model_qa.generate(\n                    **inputs2,\n                    max_new_tokens=max_new_tokens,\n                    **kwargs\n                )\n            output = self.tokenizer_qa.decode(output2[0, inputs2['input_ids'].shape[1]:],\n                                         skip_special_tokens=True)\n            if \"Ответ:\" in output:\n                output = output.replace(\"Ответ:\", \"\")\n                return \"<think>\" + prompt + \"</think>\" + output\n            prompt += output\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cascade_model = TwoStageModel(\"AccessAndrei/qwen3-1.7b-merged\", \"annodomini704/qwen3-1.7B_SoT_QA\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cascade_model.generate(\"У Кати было 2 яблока, у Пети было 3 яблока, а у Коли 4 яблока. Сколько всего было яблок у детей?\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluate on MERA datasets","metadata":{}},{"cell_type":"code","source":"def cut_answer(line):\n    if \"</think>\" not in line:\n        return line\n    return  re.sub(r'[^\\w\\s]', '', line.split(\"</think>\")[1]).strip()\n\ndef count_acc(model, dataset):\n    good_predicts = 0\n    for i in trange(len(dataset)):\n        if cut_answer(model.generate(dataset[i][\"input\"])).strip() == dataset[i][\"output\"]:\n            good_predicts += 1\n        if i%10==0:\n            print(i, good_predicts / (i+1))\n    \n    return good_predicts / len(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T08:02:31.369847Z","iopub.execute_input":"2025-07-28T08:02:31.370181Z","iopub.status.idle":"2025-07-28T08:02:31.379547Z","shell.execute_reply.started":"2025-07-28T08:02:31.370158Z","shell.execute_reply":"2025-07-28T08:02:31.378655Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"lcs = datasets.load_dataset(\"MERA-evaluation/MERA\", \"lcs\", split=\"public_test\")\ndef make_input(el):\n    input = el[\"instruction\"].replace(\"{inputs}\", el[\"inputs\"])\n    return {\"input\": input, \"output\": el[\"outputs\"]}\n\nlcs_prepared = lcs.map(make_input, remove_columns=lcs.column_names, batched=False)\ncount_acc(cascade_model, lcs_prepared)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rwsd = datasets.load_dataset(\"MERA-evaluation/MERA\", \"rwsd\", split=\"train\")\ndef make_input(el):\n    input = el[\"instruction\"].replace(\"{text}\", el[\"inputs\"][\"text\"])\n    input = input.replace(\"{span1_text}\", el[\"inputs\"][\"span1_text\"])\n    input = input.replace(\"{span2_text}\", el[\"inputs\"][\"span2_text\"])\n    return {\"input\": input, \"output\": el[\"outputs\"]}\n\nrwsd_prepared = rwsd.map(make_input, remove_columns=rwsd.column_names, batched=False)\ncount_acc(cascade_model, rwsd_prepared)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}